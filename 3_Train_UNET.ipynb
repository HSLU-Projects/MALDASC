{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f00419dc-b340-41bc-b3f6-c59c5eee4c3c",
   "metadata": {},
   "source": [
    "# Train UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8f29d40-94d2-44cd-832b-a9de2fb4d38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200fbf56-f94d-42e4-9a9a-b66bec6a0662",
   "metadata": {},
   "source": [
    "## Third Pary Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2872c50-853c-47aa-8f0b-2f45ac95ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is copied from previous_work/models/unet/UNet.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class UNetDoubleConvBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels) -> None:\n",
    "        super().__init__()\n",
    "        self.double_conv_block = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=out_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv_block(x)\n",
    "\n",
    "\n",
    "class UNetDownwardLayer(torch.nn.Module):\n",
    "    def __init__(self, *, in_channels, out_channels) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_block = UNetDoubleConvBlock(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "        )\n",
    "        self.down_sample = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_forward = self.conv_block(x)\n",
    "        out_downward = self.down_sample(out_forward)\n",
    "        return out_forward, out_downward\n",
    "\n",
    "\n",
    "class UNetUpwardLayer(torch.nn.Module):\n",
    "    def __init__(self, *, num_features) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_block = UNetDoubleConvBlock(\n",
    "            in_channels=num_features * 2,\n",
    "            out_channels=num_features,\n",
    "        )\n",
    "        self.up_sample_conv = nn.ConvTranspose2d(\n",
    "            in_channels=num_features * 2,\n",
    "            out_channels=num_features,\n",
    "            kernel_size=2,\n",
    "            stride=2,\n",
    "        )\n",
    "\n",
    "    def forward(self, x_from_lower_layer, x_from_encoder_forward):\n",
    "        x_upsampled = self.up_sample_conv(x_from_lower_layer)\n",
    "        x = torch.cat(\n",
    "            (x_upsampled, x_from_encoder_forward),\n",
    "            dim=1,\n",
    "        )\n",
    "        return self.conv_block(x)\n",
    "\n",
    "\n",
    "class UNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, *, in_size=(512, 512), in_channels=1, out_channels=1, init_features=64\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.encode_down_layer1 = UNetDownwardLayer(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=init_features,\n",
    "        )\n",
    "        self.encode_down_layer2 = UNetDownwardLayer(\n",
    "            in_channels=init_features,\n",
    "            out_channels=init_features * 2,\n",
    "        )\n",
    "        self.encode_down_layer3 = UNetDownwardLayer(\n",
    "            in_channels=init_features * 2,\n",
    "            out_channels=init_features * 4,\n",
    "        )\n",
    "        self.encode_down_layer4 = UNetDownwardLayer(\n",
    "            in_channels=init_features * 4,\n",
    "            out_channels=init_features * 8,\n",
    "        )\n",
    "        self.bottom_layer = UNetDoubleConvBlock(\n",
    "            in_channels=init_features * 8,\n",
    "            out_channels=init_features * 16,\n",
    "        )\n",
    "        self.decode_up_layer4 = UNetUpwardLayer(num_features=init_features * 8)\n",
    "        self.decode_up_layer3 = UNetUpwardLayer(num_features=init_features * 4)\n",
    "        self.decode_up_layer2 = UNetUpwardLayer(num_features=init_features * 2)\n",
    "        self.decode_up_layer1 = UNetUpwardLayer(num_features=init_features)\n",
    "        self.output_conv = nn.Conv2d(\n",
    "            in_channels=init_features,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        if out_channels == 1:\n",
    "            self.out_layer_func = nn.Sigmoid()\n",
    "        else:\n",
    "            self.out_layer_func = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_encode_forward1, x_encode_downward1 = self.encode_down_layer1(x)\n",
    "        x_encode_forward2, x_encode_downward2 = self.encode_down_layer2(\n",
    "            x_encode_downward1\n",
    "        )\n",
    "        x_encode_forward3, x_encode_downward3 = self.encode_down_layer3(\n",
    "            x_encode_downward2\n",
    "        )\n",
    "        x_encode_forward4, x_encode_downward4 = self.encode_down_layer4(\n",
    "            x_encode_downward3\n",
    "        )\n",
    "        x_out_bottom = self.bottom_layer(x_encode_downward4)\n",
    "        x_decode_upward4 = self.decode_up_layer4(x_out_bottom, x_encode_forward4)\n",
    "        x_decode_upward3 = self.decode_up_layer3(x_decode_upward4, x_encode_forward3)\n",
    "        x_decode_upward2 = self.decode_up_layer2(x_decode_upward3, x_encode_forward2)\n",
    "        x_out_decode = self.decode_up_layer1(x_decode_upward2, x_encode_forward1)\n",
    "        x_out_result = self.out_layer_func(self.output_conv(x_out_decode))\n",
    "        return x_out_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00973468-4a83-44a9-8862-8001852aeb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is taken from previous_work/train_unet.py\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms, InterpolationMode\n",
    "import torchvision.transforms.functional as ttf\n",
    "from PIL import Image\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, dataset_dir: str):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        with open(os.path.join(dataset_dir, \"image_names.json\"), \"r\") as f:\n",
    "            self.image_names = json.load(f)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[Image.Image, Image.Image]:\n",
    "        image_name = self.image_names[index]\n",
    "        image = Image.open(os.path.join(self.dataset_dir, \"xrays\", image_name)).convert(\"L\")\n",
    "        mask = Image.open(os.path.join(self.dataset_dir, \"masks\", image_name))\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_names)\n",
    "\n",
    "\n",
    "class Preload(Dataset):\n",
    "    \"\"\"\n",
    "    wrap a dataset to preload all items eagerly\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.data = []\n",
    "        for i in range(len(dataset)):\n",
    "            self.data.append(dataset[i])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "class TransformedDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        flip: float = None,\n",
    "        crop: float = None,\n",
    "        rotate: list = None,\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.flip = flip\n",
    "        self.crop = crop\n",
    "        self.rotate = rotate\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        image, mask = self.dataset[index]\n",
    "        image, mask = TransformedDataset.data_transform(image, mask, self.flip, self.crop, self.rotate)\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    @staticmethod\n",
    "    def data_transform(\n",
    "        image: Image.Image, mask: Image.Image = None, flip: float = None, crop: float = None, rotate: list = None\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        convert PIL Image to torch Tensor and do some augmentation\n",
    "        @param image: PIL Image\n",
    "        @param mask: PIL Image\n",
    "        @param flip: float, 0.0 ~ 1.0, probability of flip\n",
    "        @param crop: float, 0.0 ~ 1.0, probability of crop\n",
    "        @param rotate: list, [min_angle, max_angle], in degree\n",
    "        \"\"\"\n",
    "        dummy_mask = mask if mask is not None else Image.new(\"L\", image.size)\n",
    "        # resize\n",
    "        image = image.resize((256, 256), Image.BILINEAR)\n",
    "        dummy_mask = dummy_mask.resize((256, 256), Image.NEAREST)\n",
    "\n",
    "        # to tensor\n",
    "        image = ttf.to_tensor(image)  # shape(1, 256, 256)\n",
    "        dummy_mask = torch.from_numpy(np.array(dummy_mask)).long().unsqueeze(0)  # shape(1, 256, 256)\n",
    "\n",
    "        # normalize\n",
    "        image = ttf.normalize(image, [0.458], [0.173])\n",
    "\n",
    "        # flip\n",
    "        if flip is not None and random.random() < flip:\n",
    "            image = ttf.hflip(image)\n",
    "            dummy_mask = ttf.hflip(dummy_mask)\n",
    "\n",
    "        # crop\n",
    "        if crop is not None and random.random() < crop:\n",
    "            size = random.randint(128, 225)\n",
    "            i, j, h, w = transforms.RandomCrop.get_params(image, output_size=(size, size))\n",
    "            image = ttf.crop(image, i, j, h, w)\n",
    "            dummy_mask = ttf.crop(dummy_mask, i, j, h, w)\n",
    "\n",
    "            # resize\n",
    "            image = ttf.resize(image, (256, 256), InterpolationMode.BILINEAR, antialias=True)\n",
    "            dummy_mask = ttf.resize(dummy_mask, (256, 256), InterpolationMode.NEAREST, antialias=True)\n",
    "\n",
    "        # rotate\n",
    "        if rotate is not None and random.random() < 0.1:\n",
    "            angle = random.randint(rotate[0], rotate[1])\n",
    "            image = ttf.rotate(image, angle)\n",
    "            dummy_mask = ttf.rotate(dummy_mask, angle)\n",
    "\n",
    "        dummy_mask = dummy_mask.squeeze(0)\n",
    "        return image, dummy_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc76e4a-a770-4a70-8e08-3b1d649ecf97",
   "metadata": {},
   "source": [
    "## Own Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3299fd6-3e66-4f67-a259-a29d04ccda6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), 'previous_work'))\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# Set your parameters\n",
    "dataset_dir = os.path.join(os.getcwd(), 'dentex_dataset/segmentation/enumeration32')\n",
    "train_ratio = 0.8 \n",
    "batch_size = 32 \n",
    "seed = 42\n",
    "\n",
    "# Load and transform dataset\n",
    "dataset = Preload(SegmentationDataset(dataset_dir))\n",
    "dataset = TransformedDataset(dataset, flip=0.1, crop=0.1, rotate=[-10, 10])\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(len(dataset) * train_ratio)\n",
    "validation_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset, [train_size, validation_size], generator=torch.Generator().manual_seed(seed)\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "model = UNet()  # Define your Unet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c63042-8f21-4f24-8a69-ffdabf642df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unet(model, dataloader, num_epochs=10):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Initialize criterion, optimizer\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy Loss for binary segmentation\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).float()  # Convert labels to float\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            # Store labels and predictions for AP and mAP calculation\n",
    "            all_labels.append(labels.detach().cpu().numpy())\n",
    "            all_predictions.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "        epoch_loss = running_loss / len(dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}')\n",
    "\n",
    "        # Calculate AP and mAP\n",
    "        all_labels = np.concatenate(all_labels).reshape(-1, 1)  # Reshape all_labels to 2D array\n",
    "        all_predictions = np.concatenate(all_predictions).reshape(-1, 1)  # Reshape all_predictions to 2D array\n",
    "        AP = average_precision_score(all_labels, all_predictions)\n",
    "        mAP = AP.mean()\n",
    "        print(f'AP: {AP}, mAP: {mAP}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d9efb45-c3d5-4027-a74c-b8f326f0d33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: -1.356386776226176\n",
      "AP: 0.654060233541386, mAP: 0.654060233541386\n",
      "Epoch 2/10, Loss: -1.800070093257194\n",
      "AP: 0.6079126306182415, mAP: 0.6079126306182415\n",
      "Epoch 3/10, Loss: -1.8083518826247014\n",
      "AP: 0.6093541867953793, mAP: 0.6093541867953793\n",
      "Epoch 4/10, Loss: -1.7950889315890965\n",
      "AP: 0.6126072678348209, mAP: 0.6126072678348209\n",
      "Epoch 5/10, Loss: -1.7957886923750868\n",
      "AP: 0.612290863889115, mAP: 0.612290863889115\n",
      "Epoch 6/10, Loss: -1.8032530222781449\n",
      "AP: 0.6119226733580009, mAP: 0.6119226733580009\n",
      "Epoch 7/10, Loss: -1.8433690116232502\n",
      "AP: 0.6098901879759592, mAP: 0.6098901879759592\n",
      "Epoch 8/10, Loss: -1.8754253364887898\n",
      "AP: 0.6068890911472004, mAP: 0.6068890911472004\n",
      "Epoch 9/10, Loss: -1.890907586937071\n",
      "AP: 0.6063768459250577, mAP: 0.6063768459250577\n",
      "Epoch 10/10, Loss: -1.8106625647951\n",
      "AP: 0.6152941931367462, mAP: 0.6152941931367462\n"
     ]
    }
   ],
   "source": [
    "train_unet(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5de9d-dd6d-4454-b98a-b3a12254f378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
